\chapter{Resultants and Elimination Theory}

\subsection*{Application}
One can use this technique to split a robot's configuration space into critical sections within which the topology doesn't change -- Algebraic Cylindrical Decomposition.

Worked on by Anirudh.

One technique with several variations.

\section{Isolating simultaneous zeros}
\anirudh{Figure here}

$p(x, y)$ and $q(x, y)$ are bivariate polynomials in $x$ and $y$. We
seek their simultaneous zeros. The method of resultants produces a
single univariate polynomial $R(x)$ such that
\begin{align*}
  R(x) = 0
\end{align*}
if and only if $p(x, y) = 0$ and $q(x, y) = 0$ for some $y$. So, find
the roots of $R(x)$. For each of these, solve a polynomial in $y$ to
get the simultaneous roots of $p$ and $q$.

\textbf{Application:} One can use this technique to split a robot's
configuration space into critical sections within which the topology
does not change -- Algebraic Cylindrical Decomposition.

\section{Singular simultaneous zeros}

The method tells us when an overconstrained algebraic system has a
simultaneous zero. E.g. $p(x) = 0$ and $q(x) = 0$ (2 equations, 1
unknown).

\anirudh{Figure here}

This is parameter elimination, i.e. the parameter $x$ is
``removed''. Really, what we get is a resultant $R(\coeff
\pandq)$ that tells us whether or not $p$ and $q$ have a
simultaneous root. If $R(\coeff\pandq)= 0$ then yes,
otherwise no. If the coefficients are known numbers, then we just plug
in and test. If the coefficients are symbols, then the equation
$R(\coeff\pandq) = 0$ provides constraints on the coefficients that
tell us the conditions under which a simultaneous zero exists

\section{Implicitizing parametric equations}

\anirudh{Figure here}

Suppose we have a parameterized curve in $2$D
\begin{align*}
  \alpha(t) = (p(t), q(t))
\end{align*}
with $p$ and $q$ polynomials in $t$. We might want an implicit
equation $F(x, y) = 0$ for the same curve, with $F$ a polynomial in
$x$ and $y$.

Elimination theory will give this to us, basically by constructing the
system of equations
\begin{align*}
  p(t) - x &= 0 \\
  q(t) - y &= 0
\end{align*}
If we think of this as $2$ equations in $1$ unknown $t$, then we are
back in the ``singular simultaneous zeros'' scenario. In other words,
we have two polynomial equations in $t$. We think of $x$ and $y$ as
part of the coefficient set of these equations. The result is a
polynomial equation $F(x, y) = 0$ that provides constraints on $x$ and
$y$ for a simultaneous zero in $t$. In other words, it gives us  our
desired implicit equation.

\begin{example}[Very quick overview of the paper by Ponce and Kriegman]
  \textbf{Goal}: P\&K would like to recognize objects by matching
  image observables directly to 3D models (rather than build
  intermediate representations).

  \begin{itemize}
  \item Motivated by the use of planes, quadrics, superquadrics, etc. in CAD
  systems, P\&K assume that their models have rational parametric
  descriptions. Thus their models consist of surface patches described
  as
  \begin{align*}
    \vec{x}(s, t) = \frac{\sum s^it^j \vec{x}_{ij}}{\sum s^it^j w_{ij}}
  \end{align*}
  where $\vec{x}_{ij}$ and $w_{ij}$ are vectors and numbers derived
  from their model data. $\vec{x}$ is of the form $(x, y, z)$,
  i.e. $3$D. $s, t$ are the patch parameters
\item Next, P\&K define $\vec{\theta}$ to be the vector of observables
  (e.g. intensity, intensity gradients, 3D data, $\cdots$), and
  $\vec{p}$ to be the vector of viewing parameters (e.g. pose of an
  object relative to the camera, direction of the light source etc.)
\item The relationship between the observables $\vec{\theta}$ and the
  viewing parameters $\vec{p}$ depends on the surface $\vec{x}(s, t)$
  observed. For instance, intensity data depends both on the light
  source and the surface normal.

  The trick is to have enough observables and viewing parameters so
  that one can construct three such relationships, described
  implicitly by three equations that relate $s, t, \vec{\theta}$ and
  $\vec{p}$:
  \begin{align*}
    f_1(s, t, \vectheta, \vecp) &= 0 \\
    f_2(s, t, \vectheta, \vecp) &= 0 \\
    f_3(s, t, \vectheta, \vecp) &= 0
  \end{align*}
  (There are $3$ such functions for each surface patch in the model
  database.)
\item For instance, here is an example from the P\&K paper:

  \textbf{Observables}: $\vectheta = (x, y, I)$ where $I$ is the
  intensity observed in the image at image coordinates $(x, y)$.

  \textbf{Viewing Parameters}: $\vecp = (x_0, y_0, \vecl, \vecw,
  \vecu)$ where $(x_0, y_0)$ is the world origin in the camera's
  coordinate system (so won't matter), $\vecl$ is a unit vector
  describing the direction of the light source (situated at infinity),
  $\vecw$ is the unit vector describing the world orientation of the
  image X-axis, $\vecu$ is the unit vector describing the world
  orientation of the image Y-axis (Note: together $\vecw$ and $\vecu$
  have 3 dofs, and thus are often replaced by three angles $(\alpha,
  \beta, \gamma)$ that describe the orientation of the camera. For
  simplicity of presentation, we won't worry about that here.)

  The three equations relating $\vectheta$ and $\vecp$ in terms of a
  given surface patch $\vecx(s, t)$ are then
  \begin{align*}
    x &= \vecx(s, t)\cdot\vecw + x_0 \\
    y &= \vecx(s, t)\cdot\vecu + y_0 \\
    I &= \vecN \cdot \vecl
  \end{align*}
  where $\vecN$ is the surface normal at $\vecx(s, t)$, obtained in
  the usual way. (This derivation assumes a matte surface and
  orthographic projection, for simplicity.)
\item So, P\&K have three equations in $\vectheta$, $\vecp$ and $(s,
  t)$. Since the parameters $s, t$ aren't intrinsic they are a
  nuisance -- after all, the same surface can be parameterized in
  different ways.

  P\&K get rid of $s, t$ using Elimination theory. Again, this is much
  like case of ``Singular Simultaneous Zeros''. We think of the
  equations
  \begin{align*}
    f_1(s, t, \vectheta, \vecp) &= 0 \\
    f_2(s, t, \vectheta, \vecp) &= 0 \\
    f_3(s, t, \vectheta, \vecp) &= 0
  \end{align*}
  as $3$ equations in $2$ unknowns, i.e. an overconstrained system.

  Elimination theory allows us to construct a ``resultant'' equation
  in the coefficients of $s, t$ in $f_1, f_2, f_3$. $\vectheta$ and
  $\vecp$ are some of these coefficients. We thus get a single
  implicit equation
  \begin{align*}
    F \cdot (\vectheta, \vecp) = 0
  \end{align*}
  that relates $\vectheta$ and $\vecp$.

  The data $\{\vecx_{ij}\}$ and $\{w_{ij}\}$ appear inside this
  equation, but the parameters $s, t$ have been eliminated.

  In the image intensity example, we then get a standard implicit
  equation $F(x, y, I, x_0, y_0, \vecl, \vecw, \vecu) = 0$ relating
  observed intensity to camera and light source, parameterized by the
  observed surface point. In other words, $F$ is an implicit equation
  for the intensity surface $I(x, y)$ parameterized by all the other
  quantities.
\item \textbf{Beyond eliminating $s, t$, how is this useful to
    P\&K?}. They would like to do two things: Given some observed data
  \begin{enumerate}
  \item Determine which object they are looking at,
  \item Determine unknown viewing parameters (e.g. the orientation of
    the camera to the object, i.e. the pose of the object.)
  \end{enumerate}

  \textbf{Here is the approach P\&K take}:
  \begin{enumerate}
  \item First, they have a collection of models $M_1, \cdots,
    M_m$. For simplicity, let's think of each as being a single
    surface patch $\vecx_j(s, t)$. Thus, using elimination theory,
    P\&K construct a collection of implicit functions
    \begin{align*}
      F_1(\vectheta, \vecp) &= 0 \\
                            &\vdots \\
      F_m(\vectheta, \vecp) &= 0
    \end{align*}
    one for each model. $F_j$ relates observables and viewing
    parameters whenever the camera is looking at an object
    corresponding to the model $M_j$.
  \item Next, P\&K collect data. Specifically they look at an unknown
    object with unknown viewing parameters. They look at several
    points on the object, and then acquire collection of data points
    $\vectheta_1, \cdots, \vectheta_n$ (For instance, in the image
    intensity example, each triple $(x, y, I)$ as $(x, y)$ varies over
    the image is a data point.)
  \item Finally, P\&K run the following recognition algorithm:
    \begin{enumerate}
    \item For each model $M_j$, determine the viewing parameters
      $\vecp_j$ that minimize the squared error
      \begin{align*}
        E_j = \sum_{i=1}^n F_j^2(\vectheta_i, \vecp)
      \end{align*}
      (so $\vecp$ is the minimization variable.)
    \item For each model $M_j$, and each data point $\vectheta_i$, let
      $d_{ij}$ be the distance between $\vectheta_i$ and the surface
      in $\vectheta$-space defined implicitly by the equation
      $F_j(\vectheta, \vecp_j) = 0$. Then compute the cumulative
      distance
      \begin{align*}
        D_j = \sum_{i=1}^n d_{ij}
      \end{align*}
    \item Let $j_0$ be the index that minimizes $D_j$. Model $M_{j_0}$
      is reported back.
    \end{enumerate}
  \end{enumerate}
  \end{itemize}
\end{example}

\section{Brief intro to resultants}
Good reference: Sederberg, Anderson \& Goldman \textit{Implicit
  Representation of Parametric Curves and Surfaces} in Computer
Vision, Graphics and Image Processing $28, 72-84$ ($1984$).

\subsection{Motivation}
\begin{itemize}
\item Determine when a system of equations has a simultaneous zero
\item Use this, e.g. to implicitize parametric equations
\end{itemize}

Recall how this works for linear systems:
\begin{align*}
  a_{11}x_1 + a_{12}x_2 &+ \cdots + a_{1n}x_n = 0 \\
                        &\vdots \\
  a_{n1}x_1 + a_{n2}x_2 &+ \cdots + a_{nn}x_n = 0
\end{align*}
i.e. $Ax = 0$. This system has a \textit{non-trivial} solution iff $A$
is singular, i.e. iff $\det(A) = 0$. Furthermore, if we have a reduced
system ($n-1$ equations, $n$ unknowns)
\begin{align*}
  a_{11}x_1 + &\cdots + a_{1n}x_n = 0 \\
              &\vdots \\
  a_{n-1}x_1 + &\cdots + a_{n-1, n}x_n = 0
\end{align*}
then the ratio $\frac{x_i}{x_j}$ of any solution is given by
\begin{align*}
  \frac{x_i}{x_j} = (-1)^{i+j} \frac{\det(A_i)}{\det(A_j)}
\end{align*}
where
\begin{align*}
  A_i =
  \begin{bmatrix}
    a_{11} &\cdots &a_{1, i-1} &a_{1, i+1} &\cdots &a_{1n} \\
    \vdots&&&&&\vdots \\
    a_{n-1, 1} &\cdots &a_{n-1, i-1} &a_{n-1, i+1} &\cdots &a_{n-1, n}
  \end{bmatrix}
\end{align*}
i.e. $A_i$ is $A$ with its $i$th column removed ($A_i$ is a
$(n-1)\times (n-1)$ matrix.)

\begin{example}
  \begin{align*}
    x + y + z &= 0 \\
    2x + y &= 0
  \end{align*}
  $A_1 =
  \begin{bmatrix}
    1 & 1 \\
    1 & 0
  \end{bmatrix}
  $, $A_2 =
  \begin{bmatrix}
    1 & 1 \\
    2 & 0
  \end{bmatrix}
$, $A_3 =
\begin{bmatrix}
  1 & 1 \\
  2 & 1
\end{bmatrix}
$, $\det A_1 = -1$, $\det A_2 = -2$, and $\det A_3 = -1$. So this
syste, has a line of solutions of the form $c(1, -2, 1)$. (Set $z = 1$
and note $\frac{x}{z} = (-1)^4\frac{-1}{-1}$, $\frac{y}{z} = (-1)^5
\frac{-2}{-1}$). Indeed, if we did some linear algebra by hand on the
original two equations that's the result we would get as well.
\end{example}

\textbf{The big question}: How does one generalize this approach to
non-linear systems? For polynomial systems, Resultants and Elimination
Theory are the answer.

We will illustrate the approach with low degree polynomials.

\section{Sylvester's Method}

Suppose we have two univariate quadratics:
\begin{align*}
  ax^2 + bx + c &= 0 \\
  a'x^2 + b'x + c' = 0
\end{align*}
where $a \neq 0$ and $a' \neq 0$. Does this system have a simultaneous
zero? Well, here's a trick:

The system above has a simultaneous zero iff there is some $x$ such
that
$$
  \begin{bmatrix}
    a & b & c & 0 \\
    0 & a & b & c \\
    a' & b' & c' & 0 \\
    0 & a' & b' & c'
  \end{bmatrix}
                  \begin{bmatrix}
                    x^3 \\ x^2 \\ x \\ 1
                  \end{bmatrix} = 0 $$

Write this as $Q\vecx^3 = 0$ where $\vecx^3 =
\begin{bmatrix}
  x^3 \\ x^2 \\ x \\ 1
\end{bmatrix}
$. By analogy to the linear case we claim that this is possible iff
$\det Q = 0$.

If $x^3, x^2, x, 1$ really were independent variables, then this would
be clear, but they are not, so let us prove our claim,
\begin{claim}
  $Qx^3 = 0$ iff $\det Q = 0$
\end{claim}
\begin{proof}
  \begin{enumerate}
  \item Necessity is clear. After all if $Q
    \begin{bmatrix}
      x^3 \\ x^2 \\ x || 1
    \end{bmatrix} = 0
$ for some $x$, then $\det Q = 0$ by linear algebra since $
\begin{bmatrix}
  x^3 \\ x^2 \\ x \\ 1
\end{bmatrix}
$ is a non-trivial vector.
\item What about sufficiency? If $\det Q = 0$ can we show that there
  is an $x$ for which $Q\vecx^3 = 0$?

  Yes, here's how: Since $\det Q = 0$ we know that there is some
  vector $\vecv =
  \begin{bmatrix}
    v_1 \\ v_2 \\ v_3 \\ v_4
  \end{bmatrix}
$ such that $Q\vecv = 0$. Assuming that we started with two different
equations (which is easy enough to check) we know that rows $1$ and
$3$ of Q are independent, as are rows $2$ and $4$.

In other words, the linear system
\begin{align*}
  av_1 + bv_2 + cv_3 &= 0 \\
  a'v_1 + b'v_2 + c'v_3 &= 0
\end{align*}
has a line of solutions in $(v_1, v_2, v_3)$ space. In other words
$(v_1, v_2, v_3) = \alpha(r, s, t)$ where $(r, s, t)$ is the direction
vector of the line and $\alpha$ is a scalar.

Similarly, the system
\begin{align*}
  av_2 + bv_3 + cv_4 &= 0 \\
  a'v_2 + b'v_3 + c'v_4 &= 0
\end{align*}
has a line of solutions in $(v_2, v_3, v_4)$ space.

Of course, since the coefficients are the same, this line is the same
as in the previous case, just shifted by one coordinate.

In other words, $(v_2, v_3, v_4) = \beta(r, s, t)$ for some $\beta$.
So, if $Qv = 0$ then
\begin{align*}
  (v_1, v_2, v_3) &= \alpha(r, s, t) \\
  (v_2, v_3, v_4) &= \beta(r, s, t)
\end{align*}
Therefore, $v_1 = \frac{\alpha}{\beta}v_2 = \left(
  \frac{\alpha}{\beta} \right)^2v_3 = \left( \frac{\alpha}{\beta}
\right)^3v_4$. $\beta$ can't be zero, for if it were then we'd be
saying $Q
\begin{bmatrix}
  1 \\ 0 \\ 0 \\ 0
\end{bmatrix} = 0
$, which is impossible since $a$ and $a'$ are non-zero. Also, $v_4$
can't be zero, for then $v = 0$, and we're assuming $Qv = 0$ has a
non-trivial solution.

So, let's take $v_4$ to be $1$ (we can do that since any multiple of
$v$ will also satisfy $Qv = 0$).

Then our solution looks like $(x^3, x^2, x, 1)$, with $x =
\frac{\alpha}{\beta}$.


\textit{In short, if $\det Q = 0$, then there is a non-trivial
  solution to $Qv = 0$, and furthermore $v$ is of the form $
  \begin{bmatrix}
    x^3 \\ x^2 \\ x \\ 1
  \end{bmatrix}
$}
  \end{enumerate}
\end{proof}

\textbf{Terminology}: $\det Q = 0$ is called the resultant of the
original two equations
\begin{align*}
  ax^2 + bx + c &= 0 \\
  a'x^2 + b'x + c' = 0
\end{align*}

\begin{definition}[Resultant]
  A resultant of a set of polynomials is an expression involving the
  coefficients of the polynomials such that the vanishing of the
  resultant is a necessary and sufficient condition for the set of
  polynomials to have a common zero

  (It is a generalization of the idea of a determinant, which is an
  expression involving the coefficients of a set of linear equations.)
\end{definition}

Sylvester's method outlined above for two quadratic polynomials can be
generalized. More generally, Sylvester's method expresses the
resultant of two univariate polynomials of degree $m$ and $n$ as a
determinant of an $(m+n)\times (m+n)$ matrix.

\begin{numerical-example}
  \begin{align*}
    p(x) &= x^2 - 6x + 2 \\
    q(x) &= x^2 + x + 5
  \end{align*}
  $\det Q = \det
  \begin{bmatrix}
    1 & -6 & 2 & 0 \\
    0 & 1 & -6 & 2 \\
    1 & 1 & 5 & 0 \\
    0 & 1 & 1 & 5
  \end{bmatrix} = 233
$, which is non-zero. Therefore we know that $p$ and $q$ do not have a
common zero. (Indeed, we could check this directly. $p(x)$ has roots $
3 \pm \sqrt{7}$ and $q(x)$ has roots $-\frac{1}{2} \pm \frac{1}{2}\sqrt{-19}$)
\end{numerical-example}

\begin{numerical-example}
  \begin{align*}
    p(x) &= x^2 - 4x - 5 \\
    q(x) &= x^2 - 7x + 10
  \end{align*}
  (It's easy to solve this by hand. $x = 5$ is the common root.)

  $\det Q = \det
  \begin{bmatrix}
    1 & -4 & -5 & 0 \\
    0 & 1 & -4 & -5 \\
    1 & -7 & 10 & 0 \\
    0 & 1 & -7 & 10
  \end{bmatrix} = 0
  $, so $p$ and $q$ do have a common root.

  To find it, consider the partial system $
  \begin{bmatrix}
    1 & -4 & -5 & 0 \\
    0 & 1 & -4 & -5 \\
    1 & -7 & 10 & 0
  \end{bmatrix}
  \begin{bmatrix}
    x^3 \\ x^2 \\ x \\ 1
  \end{bmatrix} = 0
  $. Using our previous linear algebra result, we know that
  \begin{align*}
    x = \frac{x^3}{x^2} &= (-1)^{1+2} \frac{\det
    \begin{bmatrix}
      -4 & -5 & 0 \\
      1 & -4 & -5 \\
      -7 & 10 & 0
    \end{bmatrix}
}{\det
                \begin{bmatrix}
                  1 & -5 & 0 \\
                  0 & -4 & -5 \\
                  1 & 10 & 0
                \end{bmatrix}
                           } \\
    &= (-1) \frac{-375}{75} = 5
  \end{align*}
\end{numerical-example}

\begin{numerical-example}
  So far the coefficients have all been numbers, but suppose we had an
  unknown parameter. Then we could use the method of resultants to
  supply a constraint on the unknown parameter in order for a common
  zero to exist.

For example, if $p(x) = x^2 - 4x - 5$ and $q(x) = x^2 - 7x + c$. Then
we can ask: For what value of $c$ does this system have a common
root. In effect, we are eliminating $x$ from the system and
determining a constraint on $c$.

\begin{align*}
  \det Q = \det
  \begin{bmatrix}
    1 & -4 & -5 & 0 \\
0 & 1 & -4 & -5 \\
1 & -7 & c & 0 \\
0 & 1 & -7 & c
  \end{bmatrix} = c^2 - 2c - 80
\end{align*}
So $p$ and $q$ have a common root iff
\begin{align*}
  \det Q &= 0 \\
c^2 - 2c - 80 &= 0 \\
\end{align*}
which gives us $c = 10$ or $c = 8$. The case of $c=10$ we just saw on
the previous example yielding $x = 5$. The case of $c = -8$
corresponds to the system $x^2 - 4x - 5 = 0$ and $x^2 - 7x - 8 = 0$
which yields a simultaneous root $x = -1$.
\end{numerical-example}

\begin{numerical-example}
  So we see that what seemed like a simple test merely to decide
  whether an overconstrained system had a solution is actually quite
  powerful if some of the coefficients are left symbolic.

In this next example, we actually use the method to solve two
simultaneous equations in two unknowns.

\anirudh{Figure here}

Suppose we want to intersect the circle and the ellipse
\begin{align*}
  p(x, y) &= x^2 + y^2 - 16 = 0 \\
  q(x, y) = 9x^2 + 25y^2 - 225 = 0
\end{align*}

In order to apply the method of resultants we will think of $p$ and
$q$ as two \textit{univariate polynomials in $y$}, with coefficients
that happen to include the symbol $x$. We will construct the resultant
a la Sylvester's method. This will produce a single polynomial in $x$,
which is zero iff the original system has a common root in $y$. We
solve for $x$, then for $y$.

Here goes. Think of $p$ and $q$ as polynomials in $y$ only:
\begin{align*}
  p(y) &= y^2 + (x^2 - 16) \\
  q(y) &= 25y^2 + (9x^2 - 225) \\
  \det Q &= \det
           \begin{bmatrix}
             1 & 0 & x^2 - 16 & 0 \\
             0 & 1 & 0 & x^2 - 16 \\
             25 & 0 & 9x^2 - 225 & 0 \\
             0 & 25 & 0 & 9x^2 - 225
           \end{bmatrix} \\
  &= (175 - 16x^2)^2
\end{align*}
So $\det Q = 0$ is true iff $16x^2 = 175$ or $x = \pm
\frac{5}{4}\sqrt{7}$. What does this mean? It means that $p$ and $q$
have a simultaneous zero iff $x = \pm
\frac{5}{4}\sqrt{7}$. In effect the resultant $\det Q$ has projected
the common roots of $p(x, y)$ and $q(x, y)$ onto the X-axis:

\anirudh{Figure here}

Now plug these values of $x$ back into our system:
\begin{align*}
  p(y) &= y^2 + (x^2 - 16) = y^2 - \frac{81}{16} \\
  q(y)&= 25y^2 + (9x^2 - 225) = 25(y^2 - \frac{81}{16})
\end{align*}

So, indeed $p$ and $q$ have common roots, namely at $y = \pm
\frac{9}{4}$. Thus, we have the four intersection points
\begin{align*}
  &(- \frac{5}{4}\sqrt{7}, -\frac{9}{4}) \\
  &(+ \frac{5}{4}\sqrt{7}, -\frac{9}{4}) \\
  &(+\frac{5}{4}\sqrt{7}, +\frac{9}{4}) \\
  &(-\frac{5}{4}\sqrt{7}, +\frac{9}{4})
\end{align*}

\end{numerical-example}

\begin{numerical-example}
  Finally, let us look at an implicization example. Again, the basic
  technique is to construct resultants, just as before. This time we
  use it to eliminate a curve parameter.

  Consider the parameterized curve
  \begin{align*}
    x(t) &= 5t^2 + t + 3 \\
    y(t) &= 5t^2 - t - 1
  \end{align*}

  \textit{What is the implicit equation $F(x, y) = 0$ that describes
    this curve?}

  The trick is to look at the system
  \begin{align*}
    &5t^2 + t + (3 - x) = 0 \\
    &5t^2 - t + (-1 -y) = 0
  \end{align*}
  as two equations in one unknown, namely $t$. $x$ and $y$ simply play
  the role of symbolic coefficients, much like the symbol $c$ in
  example 3. \anirudh{Need to add a reference to example 3 here.}

  We know that the above system has a solution in $t$ precisely when
  $(x, y)$ is a point on the curve. In other words, $\det Q = 0$
  (where $\det Q$ is now a function of $x$ and $y$) iff $(x, y)$ is a
  point on the curve.

  \begin{align*}
    \det Q &= \det
             \begin{bmatrix}
               5 & 1 & 3 - x & 0 \\
               0 & 5 & 1 & 3-x \\
               5 & -1 & -1-y & 0 \\
               0 & 5 & -1 & -1-y
             \end{bmatrix} \\
    &= 5(84 + 38y - 42x + 5x^2 + 5y^2 - 10xy)
  \end{align*}
  So $\det Q = 0$ iff $5x^2 + 5y^2 - 42x + 38y - 10xy + 84 = 0$ which
  is our desired implicit equation. The discriminant $c_{x^2}c_{y^2} -
  (\frac{c_{xy}}{2})^2 = 5 * 5 - (\frac{10}{2})^2 = 0$, so this is a
  parabola -- Indeed it turns out that all $2$D quadratic
  parameterizations yield parabolas -- can't get hyperbolas or
  ellipses unless one looks at rational functions. For instance, the
  parameterization
  \begin{align*}
    x &= \frac{2Rt}{1+t^2} \\
    y  &= \frac{R(1-t^2)}{1+t^2}
  \end{align*}
  yields the circle of radius $R$.

  By the way, some simple additional computations make our techniques
  work with rational functions (just multiply through by the
  denominators.)

  \anirudh{Big figure here.}

  \textbf{Terminology and Facts:}
  \begin{itemize}
  \item The zero set of a system of polynomials with rational
    coefficients is called an \textit{algebraic set}.
  \item Resultants give us a way of converting rational parameterized
    surfaces into implicit equations.
  \item Interestingly, it is not always possible to go the other way,
    that is, take an algebraic surface $\{(x, y, z) | F(x, y, z) =
    0\}$ and construct a parameterization of that surface by rational functions.
  \end{itemize}

  Suppose someone gives us a point on the curve. Can we figure out the
  parameter $t$ corresponding to that point? Yes, using the reduced
  system

  The point $(x, y) = (7, 5)$ is on the curve (from before.) To find
  $t$, plug $(x, y)$ into the partial system
  \begin{align*}
    \begin{bmatrix}
      5 & 1 & 3-x & 0 \\
      0 & 5 & 1 & 3-x \\
      5 & -1 & -1-y & 0
    \end{bmatrix}
                      \begin{bmatrix}
                        t^3 \\ t^2 \\ t \\ 1
                      \end{bmatrix} = 0
  \end{align*}
  Then the following is true for $x = 7$ and $y=5$,
  \begin{align*}
    t = \frac{t^3}{t^2} &= (-1)^{1+2} \frac{\det
                          \begin{bmatrix}
                            -1 & 3-x & 0 \\
                            5 & 1 & 3-x \\
                            -1 & -1-y & 0
                          \end{bmatrix}
}{\det
                                        \begin{bmatrix}
                                          5 & 3-x & 0 \\
                                          0 & 1 & 3-x \\
                                          5 & -1-y & 0
                                        \end{bmatrix}
                                                     } \\
    &= - \frac{\det \begin{bmatrix}
                            -1 & -4 & 0 \\
                            5 & 1 & -4 \\
                            -1 & -6 & 0
                          \end{bmatrix}}{\det \begin{bmatrix}
                                          5 & -4 & 0 \\
                                          0 & 1 & -4 \\
                                          5 & -6 & 0
                                        \end{bmatrix}} \\
                        &= - \frac{-40}{-40} \\
    &= -1
  \end{align*}
  To double check $x(-1) = 5 - 1 + 3 = 7$ and $y(-1) = 5 + 1- 1 = 5$.
\end{numerical-example}

\section{Comments}
\label{sec:comments}

\begin{itemize}
\item These ideas generalize to higher dimensions, and higher order
  polynomials.
\item Sylvester's method, while easy to describe, is not the most
  compact way to generate the resultant of two univariate polynomials.

  A more compact representation is given by \textit{Caley's}
  method. We won't derive the general form here, but see page 76 of S,
  A, \& G. For two quadratics
  \begin{align*}
    ax^2 + bx + c &= 0 \\
    a'x^2 + b'x + c' &= 0
  \end{align*}
  the method says that one can replace our $4\times 4 $ matrix $Q$
  with a $2 \times 2$ matrix whose entries basically precompute some
  of the subdeterminants. Thus,
  \begin{equation*}
    \det Q = \det
    \begin{bmatrix}
      ab' - a'b & ac' - a'c \\
      ac' - a'c & bc' - b'c
    \end{bmatrix}
  \end{equation*}
  Also known as \textit{Bezout's} method.
\item The extension of Caley's method to three bivariate polynomials
  in $2$D is known as \textit{Dixon's method}. Dixon's method
  underlies the surface implicitization of Ponce \& Kriegman's paper.
\end{itemize}

For completeness let us revisit our first three examples, now using
Caley's method:
\begin{enumerate}
\item
  \begin{align*}
    p(x) &= x^2 - 6x + 2 \\
    q(x) &= x^2 + x + 5 \\
    Q &=
        \begin{bmatrix}
          7 & 3 \\ 3 & -22
        \end{bmatrix} \\
    \det Q &= -233
  \end{align*}
  So there are no common zeros.
\item
  \begin{align*}
    p(x) &= x^2 - 4x - 5\\
    q(x) &= x^2 - 7x + 10 \\
    Q &=
        \begin{bmatrix}
          -3 & 15 \\ 15 & -75
        \end{bmatrix} \\
    \det Q &= 0 
  \end{align*}
  So there is a common zero. Furthermore, its value is:
  \begin{align*}
    x = \frac{x}{1} = (-1)^{1+2} \frac{\det(15)}{\det(-3)} = 5
  \end{align*}
\item
  \begin{align*}
    p(x) &= x^2 - 4x - 5 \\
    q(x) &= x^2 - 7x + c \\
    Q &=
        \begin{bmatrix}
          -3 & c+5 & c+5 & -4c-35
        \end{bmatrix} \\
    \det Q &= -(c^2 - 2c - 80)
  \end{align*}
  which is what we got before (i.e. $p$ and $q$ have simultaneous
  zeros iff $c = 10$ or $c = 8$)
\end{enumerate}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
