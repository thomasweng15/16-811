\chapter{Calculus of Variations}
Worked on by Ratnesh

The calculus of variations arose in tight complicity with the development of mechanics. 
Much of the axiomatic grounding of physics in general and mechanics in particular consists of variational principles. 
Indeed, the rudiments of quantum theory may be derived from Newton's laws and variational principles. 
We will in these notes trace some of the history of the calculus of variations. 
One caution: while much of the treatment will seem mathematical, it is in fact very handwavy. 
A true and correct treatment would require us to pull out such differential geometry tools as covariant derivatives, affine connections, and n-forms. 

\subsection*{Applications}
\begin{itemize}
    \item Path optimization
       \begin{itemize}
            \item Optimal control (minimum cost trajectories)
        \end{itemize}
    \item Engineering
        \begin{itemize}
            \item Vibrating membranes
            \item Theory of elasticity
            \item Electrostatistics
        \end{itemize}
    \item Machine Vision
        \begin{itemize}
            \item Surface reconstruction
            \item Image flow (Motion and Structure from Optical Flow)
            \item Edge detection
        \end{itemize}    
\end{itemize}

\section{Introduction}
We have seen the basic principle ``To minimize $P$ is to solve $P' = 0$"

So far, we have only looked at finite-dimensional problems, that is, minimization of some function $f: \mathbb{R}^n \mapsto \mathbb{R}$. 
In such a problem, we seek the value of $n$ numbers that minimize $f$. 

What about infinite-dimensional problems, that is, problems in which $P$ depends on an infinity of numbers?

In particular, what about functionals (functions of functions)?

\bigbreak
\noindent \textbf{Example}:\\
Suppose we connect two points in the plane, $(x_0, y_0)$ and $(x_1, y_1)$ by a rectifiable curve of the form $y = y(x)$, as visualized in \autoref{fig:example_two_lost_points_swimming_in_a_plane}. 

\begin{figure}
    \centering
    \includegraphics[width=0.25\textwidth]{figures/placeholder.png}
    \caption{bla.}
    \label{fig:example_two_lost_points_swimming_in_a_plane}
\end{figure}

The length of the curve can be written as
\begin{equation*}
L(y) = \int_{x_0}^{x_1} {\sqrt{1 + (y')^2}} \ dx
\end{equation*}
where $L$ is a functional. 

Now, consider the problem: 

``Find the shortest curve between the points $(x_0, y_0)$ and $(x_1, y_1)$."

This problem asks us to to minimize the functional $L$. 
But $y$ has an infinite number of degrees of freedom. After all, there is one value of $y$ for each $x$ in $(x_0, x_1)$. 
(Constraining $y$ to be, say $\mathbb{C}^2$, reduces the degrees of freedom a bit, but there are still infinitely many of them. 
For instance, in the case of periodic functions, we can construct the Fourier series. 
There, the degrees of freedom correspond to choosing the co-efficients $\{C_j\}^\infty_{-\infty}$. 
In short, $L(y)$ depends on infinitely many parameters. 

We ``know" that the solution to the previous problem is given by a straight lines. 
But how do we know this?
And can we generalize our solution to other similar problems? 

That's what the calculus of variations is all about. 

\bigbreak

Some problems similar to the shortest curve problem are:
\begin{itemize}
    \item Shortest connecting curve on a non-planar surface (for example: on a sphere)

    \item Minimal surface of revolution generated by a connecting curve. 
    
    \item Shortest curve with a given area below it. 
    
    \item Closed curve of a givn perimeter that encloses the greatest area. 
    
    \item Shape of a string hanging from two points under the influence of gravity. 
    
    \item Path of a ray of light traveling through an inhomogeneous medium. 
    
    \item Shape of a wire that will cause a bead to move between two points in minimum time (gravity moves the bead). 
    
    \item The Soap Bubble / Soap Film Problem (that is, generalizations of the previous problem to higher dimensions. 
\end{itemize}

\bigbreak
\noindent \textbf{Caution}:
A functional may not have a minimizing solution. 
There are different reasons why this might happen. 
One analogy to keep in mind is that a function need not have a minimum either.
For instance, the function may be unbounded towards $-\infty$, or it may asymptotically approach a minimum but never reach it. 

\bigbreak
\noindent \textbf{Example}:
Consider the following problem:

``Find a curve of minimum length that has continuous curvature and that connects the points $(x_0, y_0)$ and $(x_1, y_1)$ in such a way that it is vertical at the endpoints." (visualized in \autoref{fig:exampl_vertical_endpoints})".

\begin{figure}
    \centering
    \includegraphics[width=0.25\textwidth]{figures/placeholder.png}
    \caption{bla.}
    \label{fig:exampl_vertical_endpoints}
\end{figure}

Observe that there is a set of a better and better solutions, but \underline{no best} solution. 

\section{Euler Equation}
\label{section:euler_equation}

The principle ``To minimize $P$ is to solve $P' = 0 $" suggests that if we try to minimize a functional $L(y)$ given by an integral, we will do well to solve a differential equation. 
Indeed, this intuition is correct. 
The resulting differential equation is called the \underline{Euler equation}. 
We will derive it presently. 

\noindent First, some perspective:

\underline{Recall the following:}

Suppose $f: \mathbb{R}^n \mapsto \mathbb{R}$. Whhat does it mean for $x^*$ to be a local extremum of $f$? (say a local minimum)

\begin{enumerate}
    \item Definitionally, we have that $f(x) \geq f(x^*)$ for every $x$ in some sufficiently small neighborhood of $x^*$. 
    
    \item A necessary condition is that $\nabla f(x^*) = 0$, i.e. $\frac{\partial f}{\partial x_i} (x^*) = 0$ for $i = 1, 2, ... , n$
\end{enumerate}


Now, suppose instead that we are dealing with a functional $P$, say
\begin{align*}
    P: \mathbb{C}^2(\mathbb{R}^n) &\mapsto \mathbb{R}\\
    f &\mapsto P(f)
\end{align*}

What does it mean for $f^*$ to be an extremum of $P$? (say a relative minimum)

Well, we can write down conditions similar to those we just wrote down for functions:

\begin{enumerate}
    \item Definitionally, we have that $P(f) \geq P(f^*)$ for every function in some sufficiently small neighborhood of $f^*$. (We are now talking about neighborhoods in function space) 
    
    \item We really want a kind 
\end{enumerate}

\section{Mechanics}

\subsection{Lagrange equations}

\subsection{The Principle of Least Action}

\subsection{Hamiltonians / Hamilton's equations of motion}

% \section{Omitted topics}