\chapter{Solving Linear Equations}

We have discussed general methods for solving arbitrary equations. For the most part, we restricted our attention to the one-dimensional case. We also looked at a special class of equations, namely those given by polynomials. Another special class is given by linear equations. Of course, the one-dimensional case is absolutely trivial, so one naturally considers systems of equations. 

\noindent I assume that you are familiar with the basic results of linear algebra, such as the diagonalization theorems and Gaussian Elimination. This section provides a very quick review, then discusses Singular Value Decomposition (SVD).

\subsection*{Applications}

Applications for linear equations are numerous. These include, of course, Linear Programming problems. Other are the local solution of non-linear differential equations via linearization, and the determination of minima in least-squares problems. 

\newpage

\section{A quick review of important definitions and facts}

\begin{enumerate}
\item Given a linear function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and a pair of bases,
    \begin{align*}
        \mathcal{B}_1 &= \{e_1, \dots, e_n\}\text{ for }\mathbb{R}^n \\
        \mathcal{B}_2 &= \{d_1, \dots, d_m\}\text{ for }\mathbb{R}^m
    \end{align*}
    we can represent $f$ by an $m\times n$ matrix $A$. \\

    Notice that the matrix $A$ depends on the choice of bases $\mathcal{B}_1 \times \mathcal{B}_2$.
    Often we just let $\mathcal{B}_1 \times \mathcal{B}_2$ be the "obvious" bases for $\mathbb{R}^n \times \mathbb{R}^m$. 
    
    \textbf{Moral:} A matrix is a particular coordinate representation of the linear function $f$.
    
\item Given an $m \times n$ matrix $A$, we make the following definitions:
    \begin{itemize}
        \item Column space --- linear combinations of the columns of $A$
        \item Row space --- linear combinations of the rows of $A$
    \end{itemize}
    
    If we think of $A$ as defining the linear mapping 
    \begin{align*}
        A: \mathbb{R}^n &\rightarrow \mathbb{R}^m 
        \\
        \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
            &\rightarrow
            \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}
            =
            A
            \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
    \end{align*}
    then the column space of $A$ is a vector subspace of $\mathbb{R}^m$, consisting of all points in $\mathbb{R}^m$ that are image vectors under $A$. Note that the row space of $A$ is just the column space of $A^T$, the transpose of $A$. \\
    
    And we define
    \begin{itemize}
        \item Null space --- set of vectors $x$ in $\mathbb{R}_n$ such that $Ax=0$.
    \end{itemize}

    The following relationships are useful to remember:
    \begin{itemize}
        \item $\dim(\text{row space}) = \dim(\text{column space})$, either of which is called the \underline{rank} of $A$,
        \item The row space and null space are complementary (perpendicular) subspaces of $\mathbb{R}^n$. In other words, 
        \begin{align*}
            \mathbb{R}^n &= \text{row space}\oplus\text{null space} \\
            n &= \dim(\text{row space}) + \dim(\text{null space})
        \end{align*}
    \end{itemize}
    \begin{figure}[th]
        \caption{The figure shows a point $x = x_r \oplus x_n$ being mapped through a matrix $A$ such that $y=Ax$. Here $Ax = Ax_r + Ax_n = Ax_r + 0 = Ax_r$.}
        \label{fig:linear-1}
        \centering
        \input{figures/chap_02/02-row_column_null_matrix.tikz}
    \end{figure}
    See Fig.~\ref{fig:linear-1} for an illustration. 

\item Suppose $A$ is an $m\times n$ matrix, and consider the system of equations 
    \begin{align*}
        Ax = b.
    \end{align*}
    \begin{itemize}
        \item If $b$ is not an element of the column space of $A$, then we say that the system is \underline{inconsistent} (or \underline{overdetermined}). \\
        \item If $b$ is in the column space of $A$ and the null space of $A$ is non-trivial, then we say that the system is \underline{underdetermined}. In this case there is a whole family of solutions, given by the affine set
            $$x_0 + N$$
        where $x_0$ is any particular solution $Ax_0 = b$, and $N$ is the null space of $A$. \\
        \item If $A$ is a $n\times n$ square matrix we say that $A$ is \underline{singular} iff:
            \begin{description}
                \item \quad $\det(A) = 0$
                \item \quad $\text{rank}(A) < n$
                \item \quad the rows of $A$ are not linearly independent
                \item \quad the columns of $A$ are not linearly independent
                \item \quad the dimension of the null space of $A$ is non-zero
                \item \quad A is not invertible.
            \end{description}
    \end{itemize}
\end{enumerate}

\newpage
\section{A quick review of matrix decompositions}

(We will look at SVD in more detail)

\subsection{Factorizations based on elimination}

Given an $m\times n$ matrix $A$ (with $m \geq n = \text{rank}(A)$), we can write $A$ in the form 
\begin{align*}
    PA &= LDU
\end{align*}
where 
\begin{description}
    \item \quad $P$ is an $m\times m$ permutation matrix that specifies row interchanges, 
    \item \quad $L$ is an $m\times m$ square low-triangular matrix with 1's on the diagonal, 
    \item \quad $U$ is an $m\times n$ upper-triangular matrix with 1's on the diagonal, and 
    \item \quad $D$ is an $m\times m$ square diagonal matrix.
\end{description}

[More generally: we may also need to perform column interchanges on $A$, in which case we get two permutation matrices, so $P_1 A P_2 = LDU$. Yet more generally: $PA=LU$ with diagional entries of $U$ no longer required to be $1$.]

\subsubsection*{Notes}
\begin{enumerate}
    \item The entries on the diagonal of $D$ are sometimes called ``pivots" (after the Gaussian Elimination algorithm). 
    \item The product of the pivots is equal to $\pm\det(A)$ (sign depends on $P$; negative if odd row interchanges), whenever $A$ is a square matrix.
    \item If $A$ is symmetric and $P=I$, then $U=L^T$.
    \item If $A$ is symmetric positive-definite, then $U=L^T$ and the diagonal entries of $D$ are strictly positive.
\end{enumerate}

\subsubsection*{Examples}
\begin{enumerate}[label=(\alph*)]
\item $
        \begin{bmatrix}
            1 & 0 \\
            1 & 1 \\
            0 & -1
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 0 & 0 \\
            1 & 1 & 0 \\
            0 & -1 & 1
        \end{bmatrix}
        \underbrace{\begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}}_{\mathrlap{\text{Could use any value here. Use $0$ if we want $\text{rank}(D) = \text{rank}(A)$.}}}
        \begin{bmatrix}
            1 & 0 \\
            0 & 1 \\
            0 & 0
        \end{bmatrix}
    $
    
    [If $m > n$, as in this example, then the last $m-n$ rows of $U$ are zero.]
\item
        $
        \begin{bmatrix}
            1 & 1 & 0 \\
            2 & 1 & -1 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 0 \\
            2 & 1
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0 \\
            0 & -1 
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 & 0 \\
            0 & 1 & 1
        \end{bmatrix}
    $
\end{enumerate}

\subsubsection*{Algorithm}

Gaussian elimination directly yields this decomposition.

\subsubsection*{Application}

As with most decompositions/factorizations, the hope is to simplify solving the system $Ax=b$. 

Suppose $A$ is square and non-singular. Then solving $Ax=b$ really means solving $LDUx = Pb$.

In turn this entails solving two simpler problems: 
\begin{enumerate}[label=(\roman*)]
    \item $Ly = Pb$ (solve for $y$)
    \item $Ux = D^{-1}y$ (solve for $x$)
\end{enumerate} 
Each of these problems can be solved easily using forward or back substitution ($D^{-1}$ is easy to compute since $D$ is diagonal with non-zero entries).

\subsection{Factorizations based on eigenvalues}

These are the standard factorizations one learns in a linear algebra course. Two important ones are:

\begin{enumerate}[label=(\roman*)]
\item If $A$ is a square $n\times n$ matrix with $n$ linearly independent eigenvectors, then
    $$A = S\Lambda S^{-1}$$
    where $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$, and $S$ is a matrix whose columns are the eigenvectors of $A$.
    
    This factorization is not always possible. (One case in which it is possible occurs when $A$ has $n$ distinct eigenvalues.)

\item One can always decompose $A$ in Jordan form, i.e.,
    $$A = MJM^{-1}$$
    \begin{align*}
        \text{where }J &= 
        \begin{bmatrix}
            J_1 & \dots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \dots & J_s
        \end{bmatrix}
        \text{is a block matrix, such that each block }\\
    J_i &= 
    \begin{bmatrix}
        \lambda_i^1 & \dots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \dots & \lambda_i^s
    \end{bmatrix}
    \text{ with $\lambda_i$ an eigenvalue of $A$.}
    \end{align*}
    
    Here $s$ is the number of independent eigenvectors of $A$. $M$ consists of eigenvectors and ``generalized'' eigenvectors.
    
    \subsubsection*{Side note}
    
    What is a ``generalized'' eigenvector?
    Well, suppose a Jordan block is of the form 
    \[J_i = \begin{bmatrix}4 & 1 \\ 0 & 4\end{bmatrix} \tag*{($\lambda_i = 4$)} \]
    Then $x_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ is an eigenvector
    and $x_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ is a generalized eigenvector
    
    since $J_i x_1 = \lambda_i x_1$ and 
    $J_i x_2 = \underbrace{\lambda_i x_2 + x_1}_{\mathclap{\text{``not quite'' an eigenvalue.}}}$
\end{enumerate}

\subsection{Factorizations based on $A^TA$}

Let's look at two such factorizations, QR and SVD.

\begin{enumerate}[label=(\roman*)]
    \item \textbf{QR}
        
    Suppose $A$ is an $m \times n$ matrix with independent columns. We can factor $A$ as:
    \[
        A = QR \tag*{($Q$ is $m \times n$,  $R$ is $n \times n$)}
    \]
    
    $Q$ has the same column space as $A$, but its columns are orthonormal vectors. In other words, $Q^TQ = I$ ($QQ^T$ may not be $I$, if $A$ is not square). 
    
    $R$ is invertible and upper triangular. 
    
    Here are two possible algorithms for computing this factorization:
    \begin{enumerate}
        \item Use Gram-Schmidt to orthogonalize the columns of $A$. The resulting orthonormal vectors constitute the columns of $Q$. The matrix $R$ is formed by keeping track of the Gram-Schmidt operations (specifically, $R$ expresses the columns of $A$ as linear combinations of the columns of $Q$). 
        \item First, form $A^TA$. This is a positive definite symmetric matrix. (It is positive definite because the columns of $A$ are independent. 
        
        Second, compute the $LDU$ factorization of $A^TA$. One finds that $A^TA = LDL^T$, with $L$ lower-triangular and $D$ diagonal strictly positive. 
        
        Finally, let $R = D^{\frac{1}{2}}L^T$ and $Q=AR^{-1}$. 
    \end{enumerate}
    
    Unfortunately, both of these straightforward algorithms have poor numerical stability. in practice one uses a different algorithm. See \S 11.3 of NRiC for a good algorithm.
    
    \subsubsection*{Applications}
    
    \begin{enumerate}[label=(\alph*)]
        \item ``The QR algorithm'': This is an iterative method that repeatedly produces $QR$ factorizations of matrices derived from $A$, building the eigenvalues of $A$ in the process.
        \item Supposed we wish to solve an overconstrained system $Ax = b$ in the least-squares sense. The least-squares solution $\bar{c}$ is given by $\bar{x}=(A^TA)^{-1}A^Tb$, assuming the columns of $A$ are indepedent. But $Q^TQ = I$, so $\bar{x} = R^{-1}Q^Tb$. 
        
        So, we can obtain $\bar{x}$ by computing $Q^Tb$, then using backsubstitution to solve $R\bar{x} = Q^Tb$. This is numerically more stable than solving the system $A^TA\bar{x} - A^Tb$ directly.
    \end{enumerate}
    
    \item \textbf{SVD}
    
    Any $m \times n$ matrix can be factored as 
    \[
        A = U \Sigma V^T
    \]
    where 
    \begin{description}
        \item $U$ is an $m \times m$ orthogonal matrix whose columns are the eigenvectors of $AA^T$
        \item $V$ is a $n \times n$ orthogonal matrix whose columsn are the eigenvectors of $A^TA$
        \item $\Sigma$ is a diagonal $n \times n$ matrix of the form 
            \begin{align*}
                \Sigma = \begin{bmatrix}
                    \sigma_1 & & & & 0 \\
                    & \ddots & & & \\ 
                    & & \sigma_k & & \\
                    & & & \ddots & \\ 
                    0 & & & & 0
                \end{bmatrix}
            \end{align*}\\
            with $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_k > 0$ \\
            and $k = \text{rank}(A)$
    \end{description}
    
    The $\{\sigma_i\}$ are the square-roots of the eigenvalues of $A^TA$. They are called the \underline{singular values} of $A$.
    
    Fact: If $A$ is symmetric positive definite then $U=V$ and $\Sigma$ is just the eigenvalue matrix of $A$.
    
    \subsubsection*{Motivation}
    \begin{enumerate}[label=(\roman*)]
        \item The basic goal is to ``solve'' the system $Ax=b$ for all matrices $A$ and vectors $b$.
        \item A second goal is to solve $Ax=b$ using a numerically stable algorithm.
        \item A third goal is to solve $Ax=b$ in a reasonably efficient manner. For instance, we do not want to compute $A^{-1}$ using determinants.
    \end{enumerate}
    
    \subsection*{Comments}
    
    \subsubsection*{Regarding the basic goal}
    \begin{itemize}
        \item If $A$ is square and invertible we of course want the solution $x = A^{-1}b$
        \item If $A$ is underconstrained, we want the entire set of solutions. 
        \item If $A$ is overconstrained, we could simply give up. But this case arises a lot in practice, so instead we will ask for the least-squares solution. In other words, we want the $\bar{x}$ which minimizes the error $\| A\bar{x} - b \|$. Geometrically, $A\bar{x}$ is the point in the column space of $A$ closest to $b$.
    \end{itemize}
    
    \subsubsection*{Regarding stability and efficiency}

    Gaussian Elimination is reasonably efficient, but it is not numerically very stable. In particular, elimination does not deal well with nearly singular matrices. The method is not designed for overconstrained systems. Even for underconstrained systems, the method requires extra work. 
    
    The poor numerical character of elimination can be seen in a couple ways
    \begin{enumerate}
        \item The elimination process assumes a non-singular matrix. But singularity and rank in general, is a slippery concept. After all, the matrix $A$ contains continuous, possibly noisy, numbers. Yet, rank is a discrete integer. Technically, both these sets are linearly independent vectors:
        \[ 
            \left\{ 
                \begin{bmatrix} 
                    1 \\ 
                    0 \\
                    0
                \end{bmatrix},
                \begin{bmatrix} 
                    0 \\ 
                    1 \\
                    0
                \end{bmatrix},
                \begin{bmatrix} 
                    0 \\ 
                    0 \\
                    1
                \end{bmatrix}
            \right\}
            \left\{ 
                \begin{bmatrix} 
                    1.01 \\ 
                    1.00 \\
                    1.00
                \end{bmatrix},
                \begin{bmatrix} 
                    1.00 \\ 
                    1.01 \\
                    1.00
                \end{bmatrix},
                \begin{bmatrix} 
                    1.00 \\ 
                    1.00 \\
                    1.01
                \end{bmatrix}
            \right\}
        \]
        Yet, the first set seems genuinely independent while the second set seems ``almost dependent.''
        
        \item Based on elimination, one solves $Ax=b$ by solving (via forward/backsubstitution) $Ly=b$ $Ux=D^{-1}y$.
        
        If $A$ is nearly singular (e.g., if it is given by the second set, of column vectors above), then $D$ will contain near-zero entries on its diagonal, and thus $D^{-1}$ will contain large numbers. That's all fine and good. In principle one needs the large numbers to obtain a true solution. The problem is if $A$ contains noisy entries. Then the large numbers may be pure noise --- noise that dominates the true information. Furthermore, since $L$ and $U$ can be fairly arbitrary, they may distort or magnify that noise across other variables. 
    \end{enumerate}
    
\end{enumerate}



\section{SVD in more detail}

Recall that it is sometimes possible to decompose an $n \times n$ matrix $A$ as: 
\[
    A = S \Lambda S^{-1}
\]
where $\lambda$ is diagonal, containing the eigenvalues of $A$, and $S$ is a change-of-basis matrix containing the eigenvectors of $A$.

Why is this nice? 

The answer lies in the change of coordinates $y= S^{-1}x$. Instead of working with the system $Ax=b$, we can work with the system $\Lambda y = c$, where $c = S^{-1}b$. Since $Lambda$ is diagonal, we really have the trivial system
\[
    \delta_i y_i = c_i, i=1,\dots, n.
\]
$\{ \delta_i \}$ are the eigenvalues of $A$.

If this system has a solution, then another change of coordinates gives us $x$ (as $x = Sy$).

Note: Offhand, there is no reason to believe that computing $S^{-1}b$ is any easier than computing $A^{-1}b$. However, in the ideal case the eigenvectors of $A$ are orthogonal. This is true, for insteance, if $AA^T = A^TA$. In that case the columns of $S$ are orthogonal, and so we can take $S$ to be orthogonal (unitary, if complex). But then $S^{-1}=S^T$, and the problem of solving $Ax=b$ becomes very simple. 

Unfortunately, the decomposition $A=S \Lambda S^{-1}$ is not always possible. 

Worse, what do we do if $A$ is not square?

\textbf{The answer is to work with $A^TA$ and $AA^T$.}

Suppose $A$ is a $m \times n$ matrix. The previous discussion implies the following decompositions:
\begin{description}
    \item $A^TA=VDV^T$ \\ 
        $V$ is $n \times n$ orthogonal, with eigenvectors of $A^TA$. \\
        $D$ is $n \times n$ diagonal, with the (non-negative) eigenvalues of $A^TA$.
    \item $AA^T = UD' U^T$ \\
        $U$ is $m \times m$ orthogonal, with the eigenvectors of $AA^T$. \\
        $D'$ is $m \times m$ diagonal, with the (non-negative) eigenvalues of $AA^T$.
\end{description}

It turns out that $D$ and $D'$ have the same non-zero diagonal entries (except that the order might be different). 

Some further reasoning then yields the SVD decomposition (as $A = U \Sigma V^T)$.

Recall $A = U \Sigma V^T, \Sigma = \begin{bmatrix}
    \sigma_1 & & & & 0 \\
    & \ddots & & & \\ 
    & & \sigma_k & & \\
    & & & \ddots & \\ 
    0 & & & & 0
\end{bmatrix}$

\subsubsection*{Observations}

\begin{itemize}
    \item $\text{rank}(A) = \text{rank}(\Sigma) = k$
    \item $\text{colspace}(A) = $span (first $k$ columns of $U$) \\
        Note: these columns form an orthonormal basis for the colspace
    \item $\text{nullspace}(A) = $span (last $n-k$ columns of $V$) \\
        Note: these columns form an orthonormal basis for the null space
    \item Each of $U$ and $V$ is an orthonormal change-of-basis matrix. We can therefore think of $\Sigma$ is a simple mapping between two different coordinate systems that amounts to dilation in each coordinate. 
    
    If we look at the effect on a unit circle, we get the picture in Fig.~\ref{fig:linear-2}.
\end{itemize}

\begin{figure}[h]
  \centering
  \resizebox{\columnwidth}{!}{
    \input{figures/chap_02/02-svd.tikz}
  }
  \caption{SVD in more detail}
  \label{fig:linear-2}
\end{figure}


\begin{figure}
  \centering
  \label{fig:linear-3}
  \caption{SVD in more more detail}
  \resizebox{\columnwidth}{!}{
    \input{figures/chap_02/02-svd_solve_linear.tikz}
  }
\end{figure}
\underline{Observation (1) revisited}

(1) tells us that we can determine the rank of $A$ by counting the non-zero entries in $\Sigma$. 

In fact, we can do better. Recall that one of our complaints about Gaussian elimination was that it did not handle noise or nearly singular matrices well. SVD remedies this situation.

For example, suppose, that an $n \times n$ matrix $A$ is newly singular. Indeed, perhaps $A$ should be singular, but due to noisy data it is not quite singular. This will show up in $\Sigma$ as:
\begin{enumerate}
    \item All of the $n$ diagonal entries in $\Sigma$ are non-zero.
    \item Some of the diagonal entries are almost zero.
\end{enumerate}

More generally an $n \times m$ matrix $A$ may appear to have rank $k$, yet when we look at $\Sigma$ we may find that some of the singular values are very close. zero. If there are $l$ such values, then the ``true'' rank of $A$ is probably $k-l$, and we would do well to modify $\Sigma$. Specifically, we should replace the $l$ nearly zero singular values with zero. 

Geometrically, the effect of this replacement is to reduce the column space of $A$ and increase its null space. The paint is that the column space is warped along directions for which $\sigma_i \approx 0$. In effect, solutions to $Ax=b$ get pulled off to infinity (since $\frac{1}{\sigma_i} = \infty$), along vectors that are almost in the null space. So, it is better to ignore the $i$th coordinate by zeroing $\sigma_i$. 

\subsubsection*{Example}

\[
    A = \begin{bmatrix}
        1.01 & 1.00 & 1.00 \\
        1.00 & 1.01 & 1.00 \\
        1.00 & 1.00 & 1.01
    \end{bmatrix}
    \text{ yields }
    \Sigma = \begin{bmatrix}
        3.01 & 0 & 0 \\
        0 & 0.01 & 0 \\
        0 & 0 & 0.01
    \end{bmatrix}
\]

For the moment, let us supposed $A$ is a square matrix. Suppose $A: \mathbb{R}^n \rightarrow \mathbb{R}^n, \text{rank}(A) = k < n$. The following pictures skteches the way in which SVD solves the system $Ax=b$ (we will see how to do this shortly). 

%%% FIGURE PG 17 %%%

Observe:
\begin{enumerate}
    \item The system $Ax=b$ has an affine set of solutions, given by $x_0 + N$, where $x_0$ is any solution and $N$ is the null space of $A$. It is easy to describe $N$, given the SVD decomposition, $N$ is just the span of the last $n-k$ columns of $V$. 
    \item $Ax=b'$ has no solution since $b'$ is not in the column space of $A$.
    
SVD will: 
\begin{enumerate}
    \item SVD solves $Ax=b$ by determining that $x$ which is closest to the origin (i.e., the $x$ with minimum magnitude). 
    \item SVD solves $Ax=b'$ by projecting $b'$ onto the column space of $A$, obtaining $b$ then solving $Ax=b$. In other words, SVD obtains the least-squares solution. 
\end{enumerate}

\subsubsection*{So, how do we compute a solution to $Ax=b$ for all these cases?}

The cool thing is that the procedure is the same. 

\textbf{Def} Given diagonal $\Sigma$, let us write $\frac{1}{\Sigma}$ to mean the diagonal matrix whose diagonal entries are of the form: 
\[
    \left(\frac{1}{\Sigma}\right)_{ii} = \begin{cases}
                                    \frac{1}{\Sigma_{ii}},& \text{if } \Sigma_{ii} \neq 0\\
                                    0,              & \text{if } \Sigma_{ii} = 0
                                \end{cases}
\]
And, as we mentioned sometimes it is useful to set $\Sigma_{ii} = 0$ if $\Sigma_{ii} \approx 0$.

\underline{Example} 

If $\Sigma = \begin{bmatrix} 
                2 & 0 & 0 \\
                0 & 5 & 0 \\
                0 & 0 & 0
            \end{bmatrix}$

then $\frac{1}{\Sigma} = \begin{bmatrix} 
                \frac{1}{2} & 0 & 0 \\
                0 & \frac{1}{5} & 0 \\
                0 & 0 & 0
            \end{bmatrix}$
            
\textbf{Observe}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
